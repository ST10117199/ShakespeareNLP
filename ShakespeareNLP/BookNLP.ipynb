{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACT I\n",
      "\n",
      "SCENE I. Venice. A street.\n",
      "\n",
      "\n",
      " Enter Antonio, Salarino and Solanio.\n",
      "\n",
      "ANTONIO.\n",
      "In sooth I know not why I am so sad,\n",
      "It wearies me, you say it wearies you;\n",
      "But how I caught it, found it, or came by it,\n",
      "What stuff ’tis made of, whereof it is born,\n",
      "I am to learn.\n",
      "And such a want-wit sadness makes of me,\n",
      "That I have much ado to know myself.\n",
      "\n",
      "SALARINO.\n",
      "Your mind is tossing on the ocean,\n",
      "There where your argosies, with portly sail\n",
      "Like signiors and rich burghers on the flood,\n",
      "Or as it were the p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "path_to_file = 'Merchant of venace.txt'\n",
    "text = open(path_to_file, 'r').read()\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {char:i for i, char in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '%': 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ';': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'Q': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'W': 43, 'X': 44, 'Y': 45, 'Z': 46, '[': 47, ']': 48, '_': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'Æ': 76, 'œ': 77, '—': 78, '‘': 79, '’': 80, '“': 81, '”': 82, '•': 83, '™': 84}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '0': 10,\n",
       " '1': 11,\n",
       " '2': 12,\n",
       " '3': 13,\n",
       " '4': 14,\n",
       " '5': 15,\n",
       " '6': 16,\n",
       " '7': 17,\n",
       " '8': 18,\n",
       " '9': 19,\n",
       " ';': 20,\n",
       " 'A': 21,\n",
       " 'B': 22,\n",
       " 'C': 23,\n",
       " 'D': 24,\n",
       " 'E': 25,\n",
       " 'F': 26,\n",
       " 'G': 27,\n",
       " 'H': 28,\n",
       " 'I': 29,\n",
       " 'J': 30,\n",
       " 'K': 31,\n",
       " 'L': 32,\n",
       " 'M': 33,\n",
       " 'N': 34,\n",
       " 'O': 35,\n",
       " 'P': 36,\n",
       " 'Q': 37,\n",
       " 'R': 38,\n",
       " 'S': 39,\n",
       " 'T': 40,\n",
       " 'U': 41,\n",
       " 'V': 42,\n",
       " 'W': 43,\n",
       " 'X': 44,\n",
       " 'Y': 45,\n",
       " 'Z': 46,\n",
       " '[': 47,\n",
       " ']': 48,\n",
       " '_': 49,\n",
       " 'a': 50,\n",
       " 'b': 51,\n",
       " 'c': 52,\n",
       " 'd': 53,\n",
       " 'e': 54,\n",
       " 'f': 55,\n",
       " 'g': 56,\n",
       " 'h': 57,\n",
       " 'i': 58,\n",
       " 'j': 59,\n",
       " 'k': 60,\n",
       " 'l': 61,\n",
       " 'm': 62,\n",
       " 'n': 63,\n",
       " 'o': 64,\n",
       " 'p': 65,\n",
       " 'q': 66,\n",
       " 'r': 67,\n",
       " 's': 68,\n",
       " 't': 69,\n",
       " 'u': 70,\n",
       " 'v': 71,\n",
       " 'w': 72,\n",
       " 'x': 73,\n",
       " 'y': 74,\n",
       " 'z': 75,\n",
       " 'Æ': 76,\n",
       " 'œ': 77,\n",
       " '—': 78,\n",
       " '‘': 79,\n",
       " '’': 80,\n",
       " '“': 81,\n",
       " '”': 82,\n",
       " '•': 83,\n",
       " '™': 84}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '$', '%', '(', ')', ',', '-', '.', '0', '1', '2',\n",
       "       '3', '4', '5', '6', '7', '8', '9', ';', 'A', 'B', 'C', 'D', 'E',\n",
       "       'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
       "       'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b',\n",
       "       'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o',\n",
       "       'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Æ', 'œ',\n",
       "       '—', '‘', '’', '“', '”', '•', '™'], dtype='<U1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A\n",
      "C\n",
      "T\n",
      " \n",
      "I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "C\n",
      "E\n",
      "N\n",
      "E\n",
      " \n",
      "I\n",
      ".\n",
      " \n",
      "V\n",
      "e\n",
      "n\n",
      "i\n",
      "c\n",
      "e\n",
      ".\n",
      " \n",
      "A\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "e\n",
      "e\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "E\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "A\n",
      "n\n",
      "t\n",
      "o\n",
      "n\n",
      "i\n",
      "o\n",
      ",\n",
      " \n",
      "S\n",
      "a\n",
      "l\n",
      "a\n",
      "r\n",
      "i\n",
      "n\n",
      "o\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "S\n",
      "o\n",
      "l\n",
      "a\n",
      "n\n",
      "i\n",
      "o\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "N\n",
      "T\n",
      "O\n",
      "N\n",
      "I\n",
      "O\n",
      ".\n",
      "\n",
      "\n",
      "I\n",
      "n\n",
      " \n",
      "s\n",
      "o\n",
      "o\n",
      "t\n",
      "h\n",
      " \n",
      "I\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "w\n",
      "h\n",
      "y\n",
      " \n",
      "I\n",
      " \n",
      "a\n",
      "m\n",
      " \n",
      "s\n",
      "o\n",
      " \n",
      "s\n",
      "a\n",
      "d\n",
      ",\n",
      "\n",
      "\n",
      "I\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "a\n",
      "r\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      ",\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "s\n",
      "a\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "a\n",
      "r\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      ";\n",
      "\n",
      "\n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "I\n",
      " \n",
      "c\n",
      "a\n",
      "u\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "u\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      "\n",
      "\n",
      "W\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "s\n",
      "t\n",
      "u\n",
      "f\n",
      "f\n",
      " \n",
      "’\n",
      "t\n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "a\n",
      "d\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "b\n",
      "o\n",
      "r\n",
      "n\n",
      ",\n",
      "\n",
      "\n",
      "I\n",
      " \n",
      "a\n",
      "m\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      ".\n",
      "\n",
      "\n",
      "A\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "w\n",
      "a\n",
      "n\n",
      "t\n",
      "-\n",
      "w\n",
      "i\n",
      "t\n",
      " \n",
      "s\n",
      "a\n",
      "d\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "m\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      "T\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "I\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "d\n",
      "o\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "m\n",
      "y\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "A\n",
      "L\n",
      "A\n",
      "R\n",
      "I\n",
      "N\n",
      "O\n",
      ".\n",
      "\n",
      "\n",
      "Y\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      "s\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "o\n",
      "c\n",
      "e\n",
      "a\n",
      "n\n",
      ",\n",
      "\n",
      "\n",
      "T\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "a\n",
      "r\n",
      "g\n",
      "o\n",
      "s\n",
      "i\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "s\n",
      "a\n",
      "i\n",
      "l\n",
      "\n",
      "\n",
      "L\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "g\n",
      "n\n",
      "i\n",
      "o\n",
      "r\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "i\n",
      "c\n",
      "h\n",
      " \n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "l\n",
      "o\n",
      "o\n",
      "d\n",
      ",\n",
      "\n",
      "\n",
      "O\n",
      "r\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n"
     ]
    }
   ],
   "source": [
    "# Create Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "for i in char_dataset.take(500):\n",
    "     print(ind_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A\n",
      "C\n",
      "T\n",
      " \n",
      "I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "C\n",
      "E\n",
      "N\n",
      "E\n",
      " \n",
      "I\n",
      ".\n",
      " \n",
      "V\n",
      "e\n",
      "n\n",
      "i\n",
      "c\n",
      "e\n",
      ".\n",
      " \n",
      "A\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "e\n",
      "e\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "E\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "A\n",
      "n\n",
      "t\n",
      "o\n",
      "n\n",
      "i\n",
      "o\n",
      ",\n",
      " \n",
      "S\n",
      "a\n",
      "l\n",
      "a\n",
      "r\n",
      "i\n",
      "n\n",
      "o\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "S\n",
      "o\n",
      "l\n",
      "a\n",
      "n\n",
      "i\n",
      "o\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "N\n",
      "T\n",
      "O\n",
      "N\n",
      "I\n",
      "O\n",
      ".\n",
      "\n",
      "\n",
      "I\n",
      "n\n",
      " \n",
      "s\n",
      "o\n",
      "o\n",
      "t\n",
      "h\n",
      " \n",
      "I\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "w\n",
      "h\n",
      "y\n",
      " \n",
      "I\n",
      " \n",
      "a\n",
      "m\n",
      " \n",
      "s\n",
      "o\n",
      " \n",
      "s\n",
      "a\n",
      "d\n",
      ",\n",
      "\n",
      "\n",
      "I\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "a\n",
      "r\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      ",\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "s\n",
      "a\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "a\n",
      "r\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      ";\n",
      "\n",
      "\n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "I\n",
      " \n",
      "c\n",
      "a\n",
      "u\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "u\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      "\n",
      "\n",
      "W\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "s\n",
      "t\n",
      "u\n",
      "f\n",
      "f\n",
      " \n",
      "’\n",
      "t\n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "a\n",
      "d\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "b\n",
      "o\n",
      "r\n",
      "n\n",
      ",\n",
      "\n",
      "\n",
      "I\n",
      " \n",
      "a\n",
      "m\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      ".\n",
      "\n",
      "\n",
      "A\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "w\n",
      "a\n",
      "n\n",
      "t\n",
      "-\n",
      "w\n",
      "i\n",
      "t\n",
      " \n",
      "s\n",
      "a\n",
      "d\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "m\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      "T\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "I\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "d\n",
      "o\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "m\n",
      "y\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "A\n",
      "L\n",
      "A\n",
      "R\n",
      "I\n",
      "N\n",
      "O\n",
      ".\n",
      "\n",
      "\n",
      "Y\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      "s\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "o\n",
      "c\n",
      "e\n",
      "a\n",
      "n\n",
      ",\n",
      "\n",
      "\n",
      "T\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "a\n",
      "r\n",
      "g\n",
      "o\n",
      "s\n",
      "i\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "s\n",
      "a\n",
      "i\n",
      "l\n",
      "\n",
      "\n",
      "L\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "g\n",
      "n\n",
      "i\n",
      "o\n",
      "r\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "i\n",
      "c\n",
      "h\n",
      " \n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "l\n",
      "o\n",
      "o\n",
      "d\n",
      ",\n",
      "\n",
      "\n",
      "O\n",
      "r\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "for i in char_dataset.take(500):\n",
    "     print(ind_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 21 23 40  1 29  0  0 39 23 25 34 25  1 29  9  1 42 54 63 58 52 54  9\n",
      "  1 21  1 68 69 67]\n",
      "\n",
      "ACT I\n",
      "\n",
      "SCENE I. Venice. A str\n",
      "\n",
      "\n",
      "[21 23 40  1 29  0  0 39 23 25 34 25  1 29  9  1 42 54 63 58 52 54  9  1\n",
      " 21  1 68 69 67 54]\n",
      "ACT I\n",
      "\n",
      "SCENE I. Venice. A stre\n"
     ]
    }
   ],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "    \n",
    "dataset = sequences.map(create_seq_targets)\n",
    "\n",
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34/34 [==============================] - 32s 891ms/step - loss: 3.4682\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 29s 832ms/step - loss: 3.2060\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 29s 839ms/step - loss: 2.9164\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 29s 831ms/step - loss: 2.5752\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 29s 847ms/step - loss: 2.4333\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 29s 848ms/step - loss: 2.3283\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 31s 897ms/step - loss: 2.2333\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 30s 872ms/step - loss: 2.1598\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 30s 881ms/step - loss: 2.0732\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 30s 881ms/step - loss: 2.0024\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 30s 873ms/step - loss: 1.9389\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 30s 874ms/step - loss: 1.8811\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 30s 877ms/step - loss: 1.8278\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 31s 894ms/step - loss: 1.7733\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 30s 889ms/step - loss: 1.7280\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 30s 867ms/step - loss: 1.6837\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 29s 853ms/step - loss: 1.6396\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 29s 848ms/step - loss: 1.5959\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 29s 855ms/step - loss: 1.5511\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 30s 871ms/step - loss: 1.5097\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 29s 849ms/step - loss: 1.4716\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 29s 852ms/step - loss: 1.4333\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 29s 850ms/step - loss: 1.3902\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 30s 867ms/step - loss: 1.3465\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 29s 845ms/step - loss: 1.3112\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 29s 839ms/step - loss: 1.2684\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 29s 845ms/step - loss: 1.2266\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 28s 827ms/step - loss: 1.1852\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 28s 830ms/step - loss: 1.1437\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 29s 846ms/step - loss: 1.0979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e584217760>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    model.add(LSTM(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model\n",
    "  \n",
    "  \n",
    "  \n",
    " # Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026\n",
    "\n",
    "\n",
    "#Create the model\n",
    "model = create_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embed_dim=embed_dim,\n",
    "  rnn_neurons=rnn_neurons,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "\n",
    "#Train the model\n",
    "epochs = 30\n",
    "model.fit(dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowere, ’n prace I lust, you is locks\n",
      "The Project Gutenberg™ eBooks Bassanio havessen py they, acquarile to parton, me\n",
      "Tull these perastario Harry normandiage toe which yoursely.\n",
      "\n",
      " Enter Lorenzo, Gold, like as true,\n",
      "Must cound leath the fing towards metth thaw they stult I hope, in the United\n",
      "State vie that gives me hel\n",
      "readship ther shopbose have good poor loxbuets hear of a Shriot and I Jessian 3 an8Thent Lazan,\n",
      "Hawn so your leften, betther to.\n",
      "\n",
      "SALARINO.\n",
      "’Tis no on the tage, sir,\n",
      "As thild have the money out in copyanterest tome.\n",
      "\n",
      "PRINCE Ont in thous of donest.\n",
      "By hearan most dear dester. Walten, in. I think is sthe the oatility on unding tolan whom you can do you diserve year.\n",
      "\n",
      "JESSICA.\n",
      "I an Seater natise the Jew’s moss in Venice\n",
      "of forfeiture.\n",
      "\n",
      "JESSICA.\n",
      "I’ll hean your mastiblatems cer lady him, fathing gow the wifthem fateen. Thou swett not ve cellanco your witestroble I speak.\n",
      "Gor, for me, as wIA.\n",
      "Tour of my heagh to owher shill come\n",
      "With all knows the mind On which the Project Gutenbe\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('shakespeare_gen.h5') \n",
    "\n",
    "#Currently our model only expects 128 sequences at a time. We can create a new model that only expects a batch_size=1. We can create a new model with this batch size, then load our saved models weights.\n",
    "#Then call .build() on the mode\n",
    "\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "model.load_weights('shakespeare_gen.h5')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "\n",
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    "  temperature = temp\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "  return (start_seed + ''.join(text_generated))\n",
    "\n",
    "\n",
    "print(generate_text(model,\"flower\",gen_size=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
